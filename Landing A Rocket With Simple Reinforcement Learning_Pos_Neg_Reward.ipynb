{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=5000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "            new_s, r, done, info = env.step(a)\n",
    "\n",
    "            #record sessions like you did before\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "                \n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Elite Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "    \n",
    "    return elite_states,elite_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch_pos_negative(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    rewards_list = []\n",
    "    \n",
    "    for i in range(len(rewards_batch)):\n",
    "        for j in range(len(states_batch[i])):\n",
    "            elite_states.append(states_batch[i][j])\n",
    "            elite_actions.append(actions_batch[i][j])\n",
    "            if rewards_batch[i] > reward_threshold:\n",
    "                rewards_list.append(1)\n",
    "            else:\n",
    "                rewards_list.append(-1)\n",
    "    \n",
    "    return elite_states,elite_actions,rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_entropy(outputs, labels,reward):\n",
    "    batch_size = outputs.size()[0]            # batch_size\n",
    "    outputs = F.log_softmax(outputs, dim=1)   # compute the log of softmax values\n",
    "    outputs = outputs[range(batch_size), labels] # pick the values corresponding to the labels\n",
    "    outputs = outputs*reward\n",
    "    return -torch.sum(outputs)/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carry Out Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=-0.888, reward_mean=-209.8, reward_threshold=-96.2\n",
      "1: loss=-0.850, reward_mean=-245.6, reward_threshold=-110.9\n",
      "2: loss=-0.819, reward_mean=-277.5, reward_threshold=-117.9\n",
      "3: loss=-0.810, reward_mean=-233.2, reward_threshold=-100.1\n",
      "4: loss=-0.870, reward_mean=-159.5, reward_threshold=-77.9\n",
      "5: loss=-0.839, reward_mean=-137.3, reward_threshold=-79.1\n",
      "6: loss=-0.775, reward_mean=-162.8, reward_threshold=-78.8\n",
      "7: loss=-0.685, reward_mean=-162.3, reward_threshold=-85.5\n",
      "8: loss=-0.692, reward_mean=-147.6, reward_threshold=-83.0\n",
      "9: loss=-0.810, reward_mean=-138.3, reward_threshold=-78.6\n",
      "10: loss=-0.863, reward_mean=-113.9, reward_threshold=-67.2\n",
      "11: loss=-0.825, reward_mean=-110.5, reward_threshold=-64.5\n",
      "12: loss=-0.628, reward_mean=-104.0, reward_threshold=-58.8\n",
      "13: loss=-0.605, reward_mean=-92.4, reward_threshold=-49.1\n",
      "14: loss=-0.557, reward_mean=-96.6, reward_threshold=-54.2\n",
      "15: loss=-0.796, reward_mean=-103.6, reward_threshold=-37.0\n",
      "16: loss=-0.588, reward_mean=-105.4, reward_threshold=-35.9\n",
      "17: loss=-0.437, reward_mean=-123.1, reward_threshold=-46.1\n",
      "18: loss=-0.827, reward_mean=-125.4, reward_threshold=-41.6\n",
      "19: loss=-0.612, reward_mean=-79.8, reward_threshold=-11.1\n",
      "20: loss=-0.452, reward_mean=-88.5, reward_threshold=-17.5\n",
      "21: loss=-0.503, reward_mean=-71.6, reward_threshold=9.1\n",
      "22: loss=-0.515, reward_mean=-90.6, reward_threshold=-8.8\n",
      "23: loss=-0.617, reward_mean=-120.7, reward_threshold=-11.0\n",
      "24: loss=-0.404, reward_mean=-118.1, reward_threshold=-8.5\n",
      "25: loss=-0.268, reward_mean=-141.4, reward_threshold=-12.7\n",
      "26: loss=-0.398, reward_mean=-119.8, reward_threshold=-2.3\n",
      "27: loss=-0.385, reward_mean=-91.9, reward_threshold=7.3\n",
      "28: loss=-0.182, reward_mean=-90.6, reward_threshold=2.8\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "batch_size = 100\n",
    "session_size = 500\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "completion_score = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "#loss function\n",
    "#objective = nn.CrossEntropyLoss()\n",
    "objective=custom_cross_entropy\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "    #pdb.set_trace()\n",
    "    #elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    #pdb.set_trace()\n",
    "    all_states,all_actions,rewards_list=filter_batch_pos_negative(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    #pdb.set_trace()\n",
    "    tensor_states = torch.FloatTensor(all_states)\n",
    "    tensor_actions = torch.LongTensor(all_actions)\n",
    "    tensor_rewards_list = torch.FloatTensor(rewards_list)\n",
    "    \n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions,tensor_rewards_list)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_threshold=%.1f\" % (\n",
    "            i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "generate_batch(env, 1, t_max=5000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(net, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
